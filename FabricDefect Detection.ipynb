{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "458bcca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "source_dir = \"C:/Users/User/Documents/sem 2 y3/ML/project/WFDD\"\n",
    "target_dir = 'fabric_classification_dataset'\n",
    "\n",
    "# Create target folders\n",
    "os.makedirs(os.path.join(target_dir, 'good'), exist_ok=True)\n",
    "os.makedirs(os.path.join(target_dir, 'defect'), exist_ok=True)\n",
    "\n",
    "# Loop through each cloth type (e.g., grey cloth, grid cloth, etc.)\n",
    "for cloth in os.listdir(source_dir):\n",
    "    cloth_path = os.path.join(source_dir, cloth)\n",
    "    \n",
    "    # -------- GOOD IMAGES --------\n",
    "    for good_subfolder in ['train/good', 'test/good']:\n",
    "        good_path = os.path.join(cloth_path, good_subfolder)\n",
    "        if os.path.exists(good_path):\n",
    "            for file in os.listdir(good_path):\n",
    "                src = os.path.join(good_path, file)\n",
    "                dst = os.path.join(target_dir, 'good', f\"{cloth}_{file}\")\n",
    "                shutil.copy(src, dst)\n",
    "\n",
    "    # -------- DEFECT IMAGES --------\n",
    "    test_path = os.path.join(cloth_path, 'test')\n",
    "    if os.path.exists(test_path):\n",
    "        for sub in os.listdir(test_path):\n",
    "            sub_path = os.path.join(test_path, sub)\n",
    "\n",
    "            if os.path.isdir(sub_path) and sub.lower() not in ['good']:\n",
    "                # It's a defect folder like contaminated, flecked, etc.\n",
    "                for file in os.listdir(sub_path):\n",
    "                    src = os.path.join(sub_path, file)\n",
    "                    dst = os.path.join(target_dir, 'defect', f\"{cloth}_{sub}_{file}\")\n",
    "                    shutil.copy(src, dst)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82236722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Lambda, Reshape, Conv2D, Multiply, Add\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def coordinate_attention(inputs, reduction=32):\n",
    "    h, w, c = inputs.shape[1:]\n",
    "\n",
    "    # Average pooling along spatial dimensions\n",
    "    avg_pool_h = tf.reduce_mean(inputs, axis=2, keepdims=True)\n",
    "    avg_pool_w = tf.reduce_mean(inputs, axis=1, keepdims=True)\n",
    "\n",
    "    avg_pool_h = Reshape((h, 1, c))(avg_pool_h)\n",
    "    avg_pool_w = Reshape((1, w, c))(avg_pool_w)\n",
    "\n",
    "    concat = Add()([avg_pool_h, avg_pool_w])\n",
    "    reduced = Conv2D(c // reduction, kernel_size=1, activation='relu', padding='same')(concat)\n",
    "\n",
    "    attn_h = Conv2D(c, kernel_size=1, activation='sigmoid', padding='same')(reduced)\n",
    "    attn_w = Conv2D(c, kernel_size=1, activation='sigmoid', padding='same')(reduced)\n",
    "\n",
    "    out = Multiply()([inputs, attn_h, attn_w])\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67dd00b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "\n",
    "def build_resnet50_with_ca(input_shape=(224, 224, 3)):\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False  # freeze pretrained layers\n",
    "\n",
    "    x = base_model.output\n",
    "    x = coordinate_attention(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0947788d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_gen.classes),\n",
    "    y=train_gen.classes\n",
    ")\n",
    "\n",
    "class_weights_dict = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "752d2364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3265 images belonging to 2 classes.\n",
      "Found 815 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "dataset_dir = 'fabric_classification_dataset'\n",
    "img_size = (224, 224)\n",
    "batch_size = 32\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2,\n",
    "    rotation_range=10,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "train_gen = train_datagen.flow_from_directory(\n",
    "    dataset_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_gen = train_datagen.flow_from_directory(\n",
    "    dataset_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    subset='validation',\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "375f432b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GOOD images: 3657\n",
      "Total DEFECT images: 423\n"
     ]
    }
   ],
   "source": [
    "print(\"Total GOOD images:\", len(os.listdir(os.path.join(target_dir, 'good'))))\n",
    "print(\"Total DEFECT images:\", len(os.listdir(os.path.join(target_dir, 'defect'))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc283cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c8e7e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "103/103 [==============================] - 313s 3s/step - loss: 0.6652 - accuracy: 0.6083 - val_loss: 0.6209 - val_accuracy: 0.6086\n",
      "Epoch 2/10\n",
      "103/103 [==============================] - 281s 3s/step - loss: 0.6440 - accuracy: 0.5911 - val_loss: 0.6539 - val_accuracy: 0.6074\n",
      "Epoch 3/10\n",
      "103/103 [==============================] - 282s 3s/step - loss: 0.6358 - accuracy: 0.5850 - val_loss: 0.6150 - val_accuracy: 0.6086\n",
      "Epoch 4/10\n",
      "103/103 [==============================] - 279s 3s/step - loss: 0.6334 - accuracy: 0.5822 - val_loss: 0.6512 - val_accuracy: 0.6049\n",
      "Epoch 5/10\n",
      "103/103 [==============================] - 281s 3s/step - loss: 0.6281 - accuracy: 0.5841 - val_loss: 0.6229 - val_accuracy: 0.6074\n",
      "Epoch 6/10\n",
      "103/103 [==============================] - 282s 3s/step - loss: 0.6315 - accuracy: 0.5749 - val_loss: 0.5990 - val_accuracy: 0.6086\n",
      "Epoch 7/10\n",
      "103/103 [==============================] - 284s 3s/step - loss: 0.6244 - accuracy: 0.5807 - val_loss: 0.5749 - val_accuracy: 0.6086\n",
      "Epoch 8/10\n",
      "103/103 [==============================] - 284s 3s/step - loss: 0.6190 - accuracy: 0.5844 - val_loss: 0.5911 - val_accuracy: 0.6086\n",
      "Epoch 9/10\n",
      "103/103 [==============================] - 281s 3s/step - loss: 0.6262 - accuracy: 0.5819 - val_loss: 0.6167 - val_accuracy: 0.6049\n",
      "Epoch 10/10\n",
      "103/103 [==============================] - 281s 3s/step - loss: 0.6351 - accuracy: 0.5449 - val_loss: 0.6162 - val_accuracy: 0.6086\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1549caa11c0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_resnet50_with_ca(input_shape=img_size + (3,))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=10,\n",
    "    class_weight=class_weights_dict,\n",
    "    callbacks=[early_stop]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "59232aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "103/103 [==============================] - 286s 3s/step - loss: 0.6257 - accuracy: 0.5847 - val_loss: 0.5777 - val_accuracy: 0.6086\n",
      "Epoch 2/5\n",
      "103/103 [==============================] - 281s 3s/step - loss: 0.6190 - accuracy: 0.5853 - val_loss: 0.5783 - val_accuracy: 0.6086\n",
      "Epoch 3/5\n",
      "103/103 [==============================] - 280s 3s/step - loss: 0.6229 - accuracy: 0.5859 - val_loss: 0.5831 - val_accuracy: 0.6074\n",
      "Epoch 4/5\n",
      "103/103 [==============================] - 282s 3s/step - loss: 0.6210 - accuracy: 0.5859 - val_loss: 0.5820 - val_accuracy: 0.6074\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x154a6edb970>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unfreeze base model\n",
    "model.layers[0].trainable = True\n",
    "\n",
    "# Re-compile with lower learning rate\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Continue training with callback\n",
    "model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=5,\n",
    "    class_weight=class_weights_dict,\n",
    "    callbacks=[early_stop]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ac86978c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 54s 2s/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Good       0.17      0.73      0.28        84\n",
      "      Defect       0.95      0.60      0.73       731\n",
      "\n",
      "    accuracy                           0.61       815\n",
      "   macro avg       0.56      0.66      0.50       815\n",
      "weighted avg       0.87      0.61      0.68       815\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "val_gen.reset()\n",
    "preds = model.predict(val_gen)\n",
    "y_pred = (preds > 0.5).astype(int).flatten()\n",
    "y_true = val_gen.classes\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=['Good', 'Defect']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "74d98d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=['Good', 'Defect'], yticklabels=['Good', 'Defect'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
